# -*- coding: utf-8 -*-
"""discriminator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YpF8KFNMZJ-Ck90nDP8q93WuGQ65XMqW
"""

import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, noise_dim=100, output_dim=784):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim),
            nn.Tanh()
        )

    def forward(self, z):
        return self.model(z)

class Discriminator(nn.Module):
    def __init__(self, input_dim=784):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Outputs probability: real vs fake
        )

    def forward(self, x):
        return self.model(x)

import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Hyperparameters
batch_size = 64
lr = 0.0002
epochs = 50
noise_dim = 100
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# MNIST loader
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1] for Tanh
])
dataloader = DataLoader(datasets.MNIST(root='./data', train=True, download=True, transform=transform),
                        batch_size=batch_size, shuffle=True)

# Initialize models
G = Generator(noise_dim).to(device)
D = Discriminator().to(device)

# Loss and optimizers
criterion = nn.BCELoss()
optimizer_G = optim.Adam(G.parameters(), lr=lr)
optimizer_D = optim.Adam(D.parameters(), lr=lr)

for epoch in range(epochs):
    for real_images, _ in dataloader:
        batch_size = real_images.size(0)
        real_images = real_images.view(batch_size, -1).to(device)

        # Create real and fake labels
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # =====================
        # Train Discriminator
        # =====================
        z = torch.randn(batch_size, noise_dim).to(device)
        fake_images = G(z)
        D_real = D(real_images)
        D_fake = D(fake_images.detach())

        D_loss_real = criterion(D_real, real_labels)
        D_loss_fake = criterion(D_fake, fake_labels)
        D_loss = D_loss_real + D_loss_fake

        optimizer_D.zero_grad()
        D_loss.backward()
        optimizer_D.step()

        # =====================
        # Train Generator
        # =====================
        z = torch.randn(batch_size, noise_dim).to(device)
        fake_images = G(z)
        D_output = D(fake_images)
        G_loss = criterion(D_output, real_labels)  # We want D to think fakes are real

        optimizer_G.zero_grad()
        G_loss.backward()
        optimizer_G.step()

    print(f"Epoch [{epoch+1}/{epochs}]  D_loss: {D_loss.item():.4f}  G_loss: {G_loss.item():.4f}")

import matplotlib.pyplot as plt

z = torch.randn(1, noise_dim).to(device)
generated = G(z).view(28, 28).detach().cpu().numpy()

plt.imshow(generated, cmap='gray')
plt.title("Sample from Generator")
plt.axis('off')
plt.show()

